{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обработка табличных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.manifold import Isomap, TSNE\n",
    "# import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_preprocessing(input_dataframe: str, \n",
    "                        numeric_columns: List[str] = [], \n",
    "                        categorical_columns: List[str] = [], \n",
    "                        target_columns: List[str] = [], \n",
    "                        ignore_columns: List[str] = [], \n",
    "                        unknown_column_action: str = 'infer',\n",
    "                        numeric_threshold: float = 0.05,\n",
    "                        numeric_scaling: str  = 'standard', \n",
    "                        categorical_encoding: str = 'one-hot',\n",
    "                        nan_action: str = 'infer', \n",
    "                        nan_threshold: float = 0.5,\n",
    "                        verbose: bool = True,\n",
    "                        save: bool = False,\n",
    "                        save_directory: str = \"./\"):\n",
    "    \"\"\"Кодирование табличных данных\n",
    "\n",
    "    Args:\n",
    "        input_dataframe (str): \n",
    "            путь до датафрейма для предобработки\n",
    "        numeric_columns (List[str], optional): \n",
    "            Defaults to [].\n",
    "        categorical_columns (List[str], optional): \n",
    "            Defaults to [].\n",
    "        target_columns (List[str], optional): \n",
    "            Defaults to [].\n",
    "        ignore_columns (List[str], optional): \n",
    "            Defaults to [].\n",
    "        unknown_column_action (str, optional): \n",
    "            Options: 'infer', 'ignore'. ignore Defaults to 'infer'.\n",
    "        numeric_threshold (float, optional):\n",
    "            Defaults to 0.05.\n",
    "        numeric_scaling (str, optional): \n",
    "            Options: 'infer', 'ignore'. Defaults to 'standard'.\n",
    "        categorical_encoding (str, optional): \n",
    "            Options: 'label', 'one-hot'. Defaults to 'one-hot'.\n",
    "        nan_action (str, optional): \n",
    "            Defaults to 'infer'.\n",
    "        nan_threshold (float, optional): \n",
    "            Defaults to 0.5.\n",
    "        verbose (bool, optional): \n",
    "            Defaults to True.\n",
    "        save (bool, optional): \n",
    "            Defaults to False.\n",
    "        save_directory (str, optional): \n",
    "            Defaults to \"./\".\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: предобработанная таблица\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"--------------------------\\nПредобработка\\n--------------------------\\n\\n\"\n",
    "        f\"\\tinput_path: {input_dataframe}, save_directory: {save_directory}\\n\"\n",
    "        f\"\\tnumeric_columns: {numeric_columns}, categorical_columns: {categorical_columns}, target_columns: {target_columns}, \\n\"\n",
    "        f\"\\tunknown_column_action: {unknown_column_action}, ignore_columns: {ignore_columns}, \\n\"\n",
    "        f\"\\tnumeric_threshold: {numeric_threshold}, numeric_scaling: {numeric_scaling}, \\n\"\n",
    "        f\"\\tcategorical_encoding: {categorical_encoding}, nan_action: {nan_action}, \\n\"\n",
    "        f\"\\tnan_threshold: {nan_threshold}, verbose: {verbose}, \\n\")\n",
    "\n",
    "    # Дописывание пустых параметров\n",
    "    if save_directory is None:\n",
    "        save_directory = './'\n",
    "    if os.path.exists(save_directory) is False:\n",
    "        os.mkdir(save_directory)\n",
    "        print(f\"{datetime.datetime.now()}: Output directory created: {save_directory}.\")\n",
    "    \n",
    "    if verbose: \n",
    "        print(f\"{datetime.datetime.now()}: Output path for the preprocessed file: {save_directory}.\")\n",
    "\n",
    "    # Загрузка датафрейма\n",
    "    if isinstance(input_dataframe, str):\n",
    "        if input_dataframe.endswith('.csv'):\n",
    "            peek_df = pd.read_csv(input_dataframe, nrows=1)\n",
    "            if peek_df.columns[0].startswith('Unnamed') or peek_df.columns[0].isdigit():\n",
    "                df = pd.read_csv(input_dataframe, index_col=0)\n",
    "            else:\n",
    "                df = pd.read_csv(input_dataframe)\n",
    "        elif input_dataframe.endswith('.xlsx'):\n",
    "            df = pd.read_excel(input_dataframe, index_col=None)\n",
    "        elif input_dataframe.endswith('.pickle'):\n",
    "            df = pd.read_pickle(input_dataframe)\n",
    "        elif input_dataframe.endswith('.json'):\n",
    "            df = pd.read_json(input_dataframe)\n",
    "        elif input_dataframe.endswith('.parquet'):\n",
    "            df = pd.read_parquet(input_dataframe)\n",
    "        elif input_dataframe.endswith('.hdf') or input_dataframe.endswith('.h5'):\n",
    "            df = pd.read_hdf(input_dataframe)\n",
    "        else:\n",
    "            supported_formats = \", \".join([\"CSV\", \"Excel (.xlsx)\", \"Pickle\", \"JSON\", \"Parquet\", \"HDF5 (.hdf, .h5)\"])\n",
    "            raise ValueError(f\"The file format is not supported. Please convert your file to one of the following supported formats: {supported_formats}.\")\n",
    "    elif isinstance(input_dataframe, pd.DataFrame):\n",
    "        df = input_dataframe.copy()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid input_path. Must be a path to a file or a pandas DataFrame.\")\n",
    "\n",
    "    # Checking columns\n",
    "    ## Checking target_columns\n",
    "    if target_columns is not None:\n",
    "        if type(target_columns) != list:\n",
    "            if target_columns in df.columns is False:\n",
    "                raise ValueError(f\"Target column {target_columns} not found.\") \n",
    "            target_columns = [target_columns] # We need them to be lists. \n",
    "        else:\n",
    "            for target_col in target_columns:\n",
    "                if target_col in df.columns is False:\n",
    "                    raise ValueError(f\"Target column {target_col} not found.\") \n",
    "    else:\n",
    "        target_columns = []\n",
    "\n",
    "    ## Checking numeric_columns\n",
    "    if numeric_columns is not None:\n",
    "        if type(numeric_columns) != list:\n",
    "            if numeric_columns in df.columns is False:\n",
    "                raise ValueError(f\"Numeric column {numeric_columns} not found.\") \n",
    "            numeric_columns = [numeric_columns] # We need them to be lists. \n",
    "        else:\n",
    "            for numeric_col in numeric_columns:\n",
    "                if numeric_col in df.columns is False:\n",
    "                    raise ValueError(f\"Numeric column {numeric_col} not found.\") \n",
    "    else:\n",
    "        numeric_columns = []\n",
    "\n",
    "    ## Checking categorical_columns\n",
    "    if categorical_columns is not None:\n",
    "        if type(categorical_columns) != list:\n",
    "            if categorical_columns in df.columns is False:\n",
    "                raise ValueError(f\"Categorical column {categorical_columns} not found.\") \n",
    "            categorical_columns = [categorical_columns] # We need them to be lists. \n",
    "        else:\n",
    "            for categorical_col in categorical_columns:\n",
    "                if categorical_col in df.columns is False:\n",
    "                    raise ValueError(f\"Categorical column {categorical_col} not found.\") \n",
    "    else:\n",
    "        categorical_columns = []\n",
    "\n",
    "    ## Checking ignore_columns\n",
    "    if ignore_columns is not None:\n",
    "        if type(ignore_columns) != list:\n",
    "            if ignore_columns in df.columns is False:\n",
    "                raise ValueError(f\"Ignore column {ignore_columns} not found.\") \n",
    "            ignore_columns = [ignore_columns] # We need them to be lists. \n",
    "        else:\n",
    "            for ignore_col in ignore_columns:\n",
    "                if ignore_col in df.columns is False:\n",
    "                    raise ValueError(f\"Ignore column {ignore_col} not found.\") \n",
    "    else:\n",
    "        ignore_columns = []\n",
    "\n",
    "    # Targets should not be preprocessed\n",
    "    ignore_columns += target_columns\n",
    "    \n",
    "    # Unknown columns inference\n",
    "    if unknown_column_action == 'infer':\n",
    "        for col in df.columns:\n",
    "            if col not in numeric_columns and col not in categorical_columns and col not in ignore_columns:\n",
    "                if df[col].dtype in [np.float64, np.float32, np.int64, np.int32]:\n",
    "                    numeric_columns.append(col)\n",
    "                    if verbose:\n",
    "                        print(f\"{datetime.datetime.now()}: Column '{col}' added to numeric columns by inference.\")\n",
    "                elif df[col].dtype == 'bool' or np.issubdtype(df[col].dtype, np.datetime64):\n",
    "                    ignore_columns.append(col)\n",
    "                    if verbose:\n",
    "                        print(f\"{datetime.datetime.now()}: Column '{col}' added to ignored columns by inference.\")\n",
    "                elif df[col].dtype == 'object':\n",
    "                    categorical_columns.append(col)\n",
    "                    if verbose:\n",
    "                        print(f\"{datetime.datetime.now()}: Column '{col}' added to categorical column columns by inference.\")      \n",
    "                else:\n",
    "                    unique_ratio = len(df[col].unique()) / len(df[col])\n",
    "                    if unique_ratio > numeric_threshold:\n",
    "                        numeric_columns.append(col)\n",
    "                        if verbose:\n",
    "                            print(f\"{datetime.datetime.now()}: Column '{col}' added to numeric columns by unique ratio inference.\")\n",
    "                    else:\n",
    "                        categorical_columns.append(col)\n",
    "                        if verbose:\n",
    "                            print(f\"{datetime.datetime.now()}: Column '{col}' added to categorical columns by unique ratio inference.\")\n",
    "    elif unknown_column_action == 'ignore':\n",
    "        ignore_columns += [col for col in df.columns if col not in numeric_columns and col not in categorical_columns and col not in ignore_columns]\n",
    "    else: \n",
    "        raise ValueError(f\"unknown_column_action {unknown_column_action} not supported. Aborting...\")\n",
    "    if verbose:\n",
    "        print(f\"--------------------------\\nDataframe short report\\n--------------------------\\n\\n\")\n",
    "        print(f\"{df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "        print(f\"column list: {list(df.columns)}\")\n",
    "        print(f\"nans:\\n{df.isna().sum()}\")\n",
    "\n",
    "    # Set target columns to be only one colum\n",
    "    target_col_name = tuple(target_columns) if len(target_columns) > 1 else (target_columns[0] if len(target_columns) == 1 else '')\n",
    "    if len(target_columns) > 1:\n",
    "        df[target_col_name] = df[target_columns].apply(tuple, axis=1)\n",
    "        df = df.drop(columns=target_columns)\n",
    "    \n",
    "    if len(target_columns) != 0:\n",
    "        unique_targets = np.unique(df[target_col_name].values)\n",
    "        N_col = df.shape[0]\n",
    "        print(f\"Target class proportions\")\n",
    "        for target in unique_targets:\n",
    "            n_target = df[df[target_col_name] == target].shape[0]\n",
    "            print(f\"\\t{target}: {n_target / N_col * 100}%\")\n",
    "    print(f\"--------------------------\\nEnd of the report.\")\n",
    "\n",
    "    # NaNs\n",
    "    if nan_action == 'drop row':\n",
    "        df.dropna(inplace=True)\n",
    "        if verbose:\n",
    "            print(f\"{datetime.datetime.now()}: Dropped rows with NaN values.\")\n",
    "    elif nan_action == 'drop column':\n",
    "        df.dropna(axis=1, thresh=int(nan_threshold * df.shape[0]), inplace=True)\n",
    "        if verbose:\n",
    "            print(f\"{datetime.datetime.now()}: Dropped columns with NaN values above threshold.\")\n",
    "    elif nan_action == 'infer':\n",
    "        for col in numeric_columns:\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "            if verbose:\n",
    "                print(f\"{datetime.datetime.now()}: Filled NaN values in numeric column '{col}' with mean.\")\n",
    "        for col in categorical_columns:\n",
    "            df[col] = df[col].fillna(df[col].mode()[0])\n",
    "            if verbose:\n",
    "                print(f\"{datetime.datetime.now()}: Filled NaN values in categorical column '{col}' with mode.\")\n",
    "        if verbose:\n",
    "            print(f\"{datetime.datetime.now()}: Filled NaN values with column means.\")\n",
    "\n",
    "    # Preprocessing numerical cols\n",
    "    if numeric_scaling == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif numeric_scaling == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
    "    if verbose:\n",
    "        print(f\"{datetime.datetime.now()}: Scaled numeric columns using {numeric_scaling} scaling.\")\n",
    "\n",
    "    # Preprocessing cat cols\n",
    "    if categorical_encoding == 'one-hot':\n",
    "        df = pd.get_dummies(df, columns=categorical_columns)\n",
    "    elif categorical_encoding == 'label':\n",
    "        encoder = LabelEncoder()\n",
    "        for col in categorical_columns:\n",
    "            df[col] = encoder.fit_transform(df[col])\n",
    "    if verbose:\n",
    "        print(f\"{datetime.datetime.now()}: Encoded categorical columns using {categorical_encoding} encoding.\")\n",
    "\n",
    "    # Сохранение\n",
    "    if save:\n",
    "        if save_directory.endswith('.pickle'):\n",
    "            df.to_pickle(save_directory)\n",
    "        elif save_directory.endswith('.csv'):\n",
    "            df.to_csv(save_directory, index=False)\n",
    "        elif save_directory.endswith('.xlsx'):\n",
    "            df.to_excel(save_directory, index=False)\n",
    "        elif save_directory.endswith('.json'):\n",
    "            df.to_json(save_directory, index=False)\n",
    "        elif save_directory.endswith('.parquet'):\n",
    "            df.to_parquet(save_directory, index=False)\n",
    "        elif save_directory.endswith('.hdf') or save_directory.endswith('.h5'):\n",
    "            df.to_hdf(save_directory, index=False)\n",
    "        if verbose:\n",
    "            print(f\"{datetime.datetime.now()}: Saved preprocessed DataFrame to {save_directory}.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Никита\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age             86\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             1\n",
      "Cabin          327\n",
      "Embarked         0\n",
      "dtype: int64\n",
      "\n",
      "PassengerId      int64\n",
      "Survived         int64\n",
      "Pclass           int64\n",
      "Name            object\n",
      "Sex             object\n",
      "Age            float64\n",
      "SibSp            int64\n",
      "Parch            int64\n",
      "Ticket          object\n",
      "Fare           float64\n",
      "Cabin           object\n",
      "Embarked        object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# create a Pandas DataFrame\n",
    "df = pd.read_csv(\"../data/tested.csv\")\n",
    "\n",
    "print(df.isnull().sum())\n",
    "print()\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "Предобработка\n",
      "--------------------------\n",
      "\n",
      "\tinput_path: ../data/tested.csv, save_directory: ../data/results\n",
      "\tnumeric_columns: [], categorical_columns: [], target_columns: [], \n",
      "\tunknown_column_action: infer, ignore_columns: [], \n",
      "\tnumeric_threshold: 0.05, numeric_scaling: standard, \n",
      "\tcategorical_encoding: label, nan_action: infer, \n",
      "\tnan_threshold: 0.5, verbose: True, \n",
      "\n",
      "2024-09-18 14:53:20.466447: Output directory created: ../data/results.\n",
      "2024-09-18 14:53:20.466447: Output path for the preprocessed file: ../data/results.\n",
      "2024-09-18 14:53:20.470195: Column 'PassengerId' added to numeric columns by inference.\n",
      "2024-09-18 14:53:20.470195: Column 'Survived' added to numeric columns by inference.\n",
      "2024-09-18 14:53:20.470195: Column 'Pclass' added to numeric columns by inference.\n",
      "2024-09-18 14:53:20.471660: Column 'Name' added to categorical column columns by inference.\n",
      "2024-09-18 14:53:20.471660: Column 'Sex' added to categorical column columns by inference.\n",
      "2024-09-18 14:53:20.471660: Column 'Age' added to numeric columns by inference.\n",
      "2024-09-18 14:53:20.471660: Column 'SibSp' added to numeric columns by inference.\n",
      "2024-09-18 14:53:20.471660: Column 'Parch' added to numeric columns by inference.\n",
      "2024-09-18 14:53:20.471660: Column 'Ticket' added to categorical column columns by inference.\n",
      "2024-09-18 14:53:20.471660: Column 'Fare' added to numeric columns by inference.\n",
      "2024-09-18 14:53:20.471660: Column 'Cabin' added to categorical column columns by inference.\n",
      "2024-09-18 14:53:20.471660: Column 'Embarked' added to categorical column columns by inference.\n",
      "--------------------------\n",
      "Dataframe short report\n",
      "--------------------------\n",
      "\n",
      "\n",
      "418 rows and 12 columns\n",
      "column list: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
      "nans:\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age             86\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             1\n",
      "Cabin          327\n",
      "Embarked         0\n",
      "dtype: int64\n",
      "--------------------------\n",
      "End of the report.\n",
      "2024-09-18 14:53:20.473670: Filled NaN values in numeric column 'PassengerId' with mean.\n",
      "2024-09-18 14:53:20.473670: Filled NaN values in numeric column 'Survived' with mean.\n",
      "2024-09-18 14:53:20.473670: Filled NaN values in numeric column 'Pclass' with mean.\n",
      "2024-09-18 14:53:20.474669: Filled NaN values in numeric column 'Age' with mean.\n",
      "2024-09-18 14:53:20.474669: Filled NaN values in numeric column 'SibSp' with mean.\n",
      "2024-09-18 14:53:20.474669: Filled NaN values in numeric column 'Parch' with mean.\n",
      "2024-09-18 14:53:20.474669: Filled NaN values in numeric column 'Fare' with mean.\n",
      "2024-09-18 14:53:20.476143: Filled NaN values in categorical column 'Name' with mode.\n",
      "2024-09-18 14:53:20.476143: Filled NaN values in categorical column 'Sex' with mode.\n",
      "2024-09-18 14:53:20.477150: Filled NaN values in categorical column 'Ticket' with mode.\n",
      "2024-09-18 14:53:20.478311: Filled NaN values in categorical column 'Cabin' with mode.\n",
      "2024-09-18 14:53:20.478311: Filled NaN values in categorical column 'Embarked' with mode.\n",
      "2024-09-18 14:53:20.478311: Filled NaN values with column means.\n",
      "2024-09-18 14:53:20.481326: Scaled numeric columns using standard scaling.\n",
      "2024-09-18 14:53:20.482400: Encoded categorical columns using label encoding.\n"
     ]
    }
   ],
   "source": [
    "new_df = table_preprocessing('../data/tested.csv', save_directory='../data/results', categorical_encoding='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId    0\n",
      "Survived       0\n",
      "Pclass         0\n",
      "Name           0\n",
      "Sex            0\n",
      "Age            0\n",
      "SibSp          0\n",
      "Parch          0\n",
      "Ticket         0\n",
      "Fare           0\n",
      "Cabin          0\n",
      "Embarked       0\n",
      "dtype: int64\n",
      "\n",
      "PassengerId    float64\n",
      "Survived       float64\n",
      "Pclass         float64\n",
      "Name             int32\n",
      "Sex              int32\n",
      "Age            float64\n",
      "SibSp          float64\n",
      "Parch          float64\n",
      "Ticket           int32\n",
      "Fare           float64\n",
      "Cabin            int32\n",
      "Embarked         int32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(new_df.isnull().sum())\n",
    "print()\n",
    "print(new_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.727912</td>\n",
       "      <td>-0.755929</td>\n",
       "      <td>0.873482</td>\n",
       "      <td>206</td>\n",
       "      <td>1</td>\n",
       "      <td>0.334993</td>\n",
       "      <td>-0.499470</td>\n",
       "      <td>-0.400248</td>\n",
       "      <td>152</td>\n",
       "      <td>-0.498407</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.719625</td>\n",
       "      <td>1.322876</td>\n",
       "      <td>0.873482</td>\n",
       "      <td>403</td>\n",
       "      <td>0</td>\n",
       "      <td>1.325530</td>\n",
       "      <td>0.616992</td>\n",
       "      <td>-0.400248</td>\n",
       "      <td>221</td>\n",
       "      <td>-0.513274</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.711337</td>\n",
       "      <td>-0.755929</td>\n",
       "      <td>-0.315819</td>\n",
       "      <td>269</td>\n",
       "      <td>1</td>\n",
       "      <td>2.514175</td>\n",
       "      <td>-0.499470</td>\n",
       "      <td>-0.400248</td>\n",
       "      <td>73</td>\n",
       "      <td>-0.465088</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.703050</td>\n",
       "      <td>-0.755929</td>\n",
       "      <td>0.873482</td>\n",
       "      <td>408</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.259330</td>\n",
       "      <td>-0.499470</td>\n",
       "      <td>-0.400248</td>\n",
       "      <td>147</td>\n",
       "      <td>-0.483466</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.694763</td>\n",
       "      <td>1.322876</td>\n",
       "      <td>0.873482</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.655545</td>\n",
       "      <td>0.616992</td>\n",
       "      <td>0.619896</td>\n",
       "      <td>138</td>\n",
       "      <td>-0.418471</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1.694763</td>\n",
       "      <td>-0.755929</td>\n",
       "      <td>0.873482</td>\n",
       "      <td>353</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.499470</td>\n",
       "      <td>-0.400248</td>\n",
       "      <td>267</td>\n",
       "      <td>-0.494448</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1.703050</td>\n",
       "      <td>1.322876</td>\n",
       "      <td>-1.505120</td>\n",
       "      <td>283</td>\n",
       "      <td>0</td>\n",
       "      <td>0.691586</td>\n",
       "      <td>-0.499470</td>\n",
       "      <td>-0.400248</td>\n",
       "      <td>324</td>\n",
       "      <td>1.313753</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1.711337</td>\n",
       "      <td>-0.755929</td>\n",
       "      <td>0.873482</td>\n",
       "      <td>332</td>\n",
       "      <td>1</td>\n",
       "      <td>0.651965</td>\n",
       "      <td>-0.499470</td>\n",
       "      <td>-0.400248</td>\n",
       "      <td>346</td>\n",
       "      <td>-0.508792</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1.719625</td>\n",
       "      <td>-0.755929</td>\n",
       "      <td>0.873482</td>\n",
       "      <td>384</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.499470</td>\n",
       "      <td>-0.400248</td>\n",
       "      <td>220</td>\n",
       "      <td>-0.494448</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1.727912</td>\n",
       "      <td>-0.755929</td>\n",
       "      <td>0.873482</td>\n",
       "      <td>302</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.616992</td>\n",
       "      <td>0.619896</td>\n",
       "      <td>105</td>\n",
       "      <td>-0.237906</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived    Pclass  Name  Sex       Age     SibSp     Parch  \\\n",
       "0      -1.727912 -0.755929  0.873482   206    1  0.334993 -0.499470 -0.400248   \n",
       "1      -1.719625  1.322876  0.873482   403    0  1.325530  0.616992 -0.400248   \n",
       "2      -1.711337 -0.755929 -0.315819   269    1  2.514175 -0.499470 -0.400248   \n",
       "3      -1.703050 -0.755929  0.873482   408    1 -0.259330 -0.499470 -0.400248   \n",
       "4      -1.694763  1.322876  0.873482   178    0 -0.655545  0.616992  0.619896   \n",
       "..           ...       ...       ...   ...  ...       ...       ...       ...   \n",
       "413     1.694763 -0.755929  0.873482   353    1  0.000000 -0.499470 -0.400248   \n",
       "414     1.703050  1.322876 -1.505120   283    0  0.691586 -0.499470 -0.400248   \n",
       "415     1.711337 -0.755929  0.873482   332    1  0.651965 -0.499470 -0.400248   \n",
       "416     1.719625 -0.755929  0.873482   384    1  0.000000 -0.499470 -0.400248   \n",
       "417     1.727912 -0.755929  0.873482   302    1  0.000000  0.616992  0.619896   \n",
       "\n",
       "     Ticket      Fare  Cabin  Embarked  \n",
       "0       152 -0.498407     15         1  \n",
       "1       221 -0.513274     15         2  \n",
       "2        73 -0.465088     15         1  \n",
       "3       147 -0.483466     15         2  \n",
       "4       138 -0.418471     15         2  \n",
       "..      ...       ...    ...       ...  \n",
       "413     267 -0.494448     15         2  \n",
       "414     324  1.313753     22         0  \n",
       "415     346 -0.508792     15         2  \n",
       "416     220 -0.494448     15         2  \n",
       "417     105 -0.237906     15         0  \n",
       "\n",
       "[418 rows x 12 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
